import paramiko
import zipfile
import pickle
import os
import pandas as pd
from datetime import datetime, timedelta

def download_and_extract_all_files(hostname, port, username, password, remote_folder, local_folder):
    # Create an SSH client
    ssh = paramiko.SSHClient()

    try:
        # Automatically add the server's host key
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        # Connect to the SSH server
        ssh.connect(hostname, port, username, password)

        # Open an SFTP session
        with ssh.open_sftp() as sftp:
            # Change to the remote folder
            sftp.chdir(remote_folder)

            # List files in the remote folder
            files = sftp.listdir()

            # Dictionary to store dataframes for each date
            date_dataframes = {}

            # Iterate through files and download/extract for each date
            for file in files:
                if file.startswith("UnderlyingEOD") and not "Summaries" in file:
                    date_format = file.split("_")[-1].split(".")[0]  # Extract date from the filename
                    remote_path = os.path.join(remote_folder, file).replace("\\", "/")
                    local_path = os.path.join(local_folder, file).replace("\\", "/")

                    # Download the file
                    sftp.get(remote_path, local_path)

                    # Extract files from the downloaded ZIP file
                    extract_zip(local_path, local_folder)

                    # Read the CSV file into a DataFrame
                    csv_file = [csv for csv in os.listdir(local_folder) if csv.endswith('.csv')]
                    if csv_file:
                        csv_file_path = os.path.join(local_folder, csv_file[0])
                        eod_df = pd.read_csv(csv_file_path)

                        # Store the DataFrame in the dictionary with the date as the key
                        date_dataframes[date_format] = eod_df

            # Clean up: Remove downloaded ZIP files
            for file in os.listdir(local_folder):
                file_path = os.path.join(local_folder, file)
                if file_path.endswith(".zip"):
                    os.remove(file_path)

    except Exception as e:
        print(f"Error: {e}")

    finally:
        # Close the SSH connection
        ssh.close()

    return date_dataframes

def extract_zip(zip_filename, extraction_folder):
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        # Extract all files in the ZIP archive to the extraction folder
        zip_ref.extractall(extraction_folder)

# SFTP server details
sftp_hostname = "sftp.datashop.livevol.com"
sftp_port = 22  # Change the port if your SFTP server uses a different port
sftp_username = "nan2_lehigh_edu"
sftp_password = "PAIndex2023!"
sftp_remote_folder = "/subscriptions/order_000046197/item_000053507"  # Change to the actual path on the SFTP server
local_download_folder = os.path.join(os.getcwd(), "input")  # Set to "input" folder inside the current directory

# Create the local download folder if it doesn't exist
os.makedirs(local_download_folder, exist_ok=True)

# Download all files and extract for each date
date_dataframes = download_and_extract_all_files(sftp_hostname, sftp_port, sftp_username, sftp_password, sftp_remote_folder, local_download_folder)


local_download_folder = os.path.join(os.getcwd(), "input")  # Set to "input" folder inside the current directory

# Create a dictionary to store dataframes for each date
date_dataframes = {}

# Loop through all files in the "input" folder
for file in os.listdir(local_download_folder):
    if file.startswith("UnderlyingEOD_") and file.endswith(".csv"):
        # Extract date from the filename
        date_format = file.split("_")[-1].split(".")[0]

        # Create the full file path
        csv_file_path = os.path.join(local_download_folder, file)

        # Read the CSV file into a DataFrame
        eod_df = pd.read_csv(csv_file_path)

        # Store the DataFrame in the dictionary with the date as the key
        date_dataframes[date_format] = eod_df

        # After using the CSV file, optionally delete it
        try:
            os.remove(csv_file_path)
            print(f"Deleted: {csv_file_path}")
        except FileNotFoundError:
            print(f"File not found: {csv_file_path}")
        except Exception as e:
            print(f"Error deleting file: {e}")

# Now, you can access each date's DataFrame using date_dataframes[date_format]
date_dataframes.pop(date_format)
output_file_path = os.path.join(os.getcwd(), r"input\date_dataframes.pkl")
with open(output_file_path, 'wb') as output_file:
    pickle.dump(date_dataframes, output_file)

print(f"Saved date_dataframes to: {output_file_path}")


#date_dataframes['2023-11-21']


df = pd.read_excel("input/RAY as of Oct 23 20231_PA.xlsx")
df.columns = df.columns.str.rstrip('\n')
df['Ticker'] = df['Ticker'].str.split(' ', n=1, expand=True)[0]
df['Ticker'] = df['Ticker'].str.replace(' ', '')  # Remove spaces
# Create float_df by selecting the first 100 rows of "Ticker" and "Equity Float" columns
float_df = df[['Ticker', 'Equity Float']].head(100)
float_df


# Create an empty DataFrame to store market cap DataFrames for each date
eod_market_cap_pivot = pd.DataFrame()

# Extract unique dates from date_dataframes dictionary
unique_dates = list(date_dataframes.keys())

# Iterate over unique dates
for date_format in unique_dates[:-1]:
    # Access the corresponding DataFrame from date_dataframes
    eod_df = date_dataframes[date_format]

    # Merge dataframes on the common column 'Ticker' and 'underlying_symbol'
    merged_df = pd.merge(float_df, eod_df, left_on='Ticker', right_on='underlying_symbol')

    # Perform the multiplication and rename the column to 'Market Cap'
    merged_df['Market Cap'] = merged_df['Equity Float'] * merged_df['close']

    # Use pivot_table to create a multi-level DataFrame
    eod_market_cap_daily = merged_df.pivot_table(index='quote_date', columns='Ticker', values='Market Cap', aggfunc='sum')

    # Add a new column for the sum of market caps for each date
    #eod_market_cap_daily['close index market cap'] = eod_market_cap_daily.sum(axis=1)

    # Append the daily market cap DataFrame to the main DataFrame
    eod_market_cap_pivot = pd.concat([eod_market_cap_pivot, eod_market_cap_daily])

# Rename the index to 'Date'
eod_market_cap_pivot = eod_market_cap_pivot.rename_axis(index='Date')

# Display the resulting pivot table
eod_market_cap_pivot.index = pd.to_datetime(eod_market_cap_pivot.index, errors='coerce')
#eod_market_cap_pivot.index.name = None  # Remove the index name

# Display the resulting pivot table
eod_market_cap_pivot
eod_market_cap_pivot.to_csv("input/LUPA100Mktcap_2023.csv")


eod_market_cap_pivot_new = pd.read_csv("input/LUPA100Mktcap_2023.csv", index_col = "Date")
eod_market_cap_pivot_new


# Create an empty DataFrame to store market cap DataFrames for each date
eod_market_cap_pivot = pd.DataFrame()

# Extract unique dates from date_dataframes dictionary
unique_dates = list(date_dataframes.keys())

# Iterate over unique dates
for date_format in unique_dates:
    # Access the corresponding DataFrame from date_dataframes
    eod_df = date_dataframes[date_format]

    # Merge dataframes on the common column 'Ticker' and 'underlying_symbol'
    merged_df = pd.merge(float_df, eod_df, left_on='Ticker', right_on='underlying_symbol')

    # Perform the multiplication and rename the column to 'Market Cap'
    merged_df['Market Cap'] = merged_df['Equity Float'] * merged_df['close']

    # Use pivot_table to create a multi-level DataFrame
    eod_market_cap_daily = merged_df.pivot_table(index='quote_date', columns='Ticker', values='Market Cap', aggfunc='sum')

    # Add a new column for the sum of market caps for each date
    eod_market_cap_daily['close index market cap'] = eod_market_cap_daily.sum(axis=1)

    # Append the daily market cap DataFrame to the main DataFrame
    eod_market_cap_pivot = pd.concat([eod_market_cap_pivot, eod_market_cap_daily])

# Rename the index to 'Date'
eod_market_cap_pivot = eod_market_cap_pivot.rename_axis(index='Date')

# Display the resulting pivot table
eod_market_cap_pivot.index = pd.to_datetime(eod_market_cap_pivot.index, errors='coerce')
#eod_market_cap_pivot.index.name = None  # Remove the index name

# Display the resulting pivot table
eod_market_cap_pivot


eod_market_cap_pivot["CMCSA"]
#eod_market_cap_pivot[["CMCSA"]]


# # Create the new columns
# eod_market_cap_pivot['gross_change_mcap'] = eod_market_cap_pivot['close index market cap'].diff().fillna(0)

# # Calculate mkt_cap_deleted_stock
# eod_market_cap_pivot['mkt_cap_deleted_stock'] = eod_market_cap_pivot.apply(
#     lambda row: row['close index market cap'] if pd.isna(row['close index market cap']) or row['close index market cap'] == 0 else 0,
#     axis=1
# )

# # Fill NaNs with 0 in the mkt_cap_deleted_stock column
# eod_market_cap_pivot['mkt_cap_deleted_stock'] = eod_market_cap_pivot['mkt_cap_deleted_stock'].fillna(0)

# # Calculate adjusted_mkt_cap
# eod_market_cap_pivot['adjusted_mkt_cap'] = eod_market_cap_pivot['close index market cap'].shift(1) - eod_market_cap_pivot['mkt_cap_deleted_stock']

# # Fill NaNs with 0 in the adjusted_mkt_cap column
# eod_market_cap_pivot['adjusted_mkt_cap'] = eod_market_cap_pivot['adjusted_mkt_cap'].fillna(0)

# # Calculate divisor
# eod_market_cap_pivot['divisor'] = eod_market_cap_pivot.apply(
#     lambda row: 1.00 if pd.isna(row['mkt_cap_deleted_stock']) or row['mkt_cap_deleted_stock'] == 0 else row['adjusted_mkt_cap'] / row['close index market cap'],
#     axis=1
# )

# # Calculate gross index level
# eod_market_cap_pivot['gross index level'] = eod_market_cap_pivot['close index market cap'] / eod_market_cap_pivot['divisor']

# # Calculate Index Value
# eod_market_cap_pivot['Index Value'] = eod_market_cap_pivot['gross index level'] / eod_market_cap_pivot['gross index level'].iloc[0] * 100

# # Display the DataFrame with the new columns
# eod_market_cap_pivot



# # Create the new columns
# eod_market_cap_pivot['gross_change_mcap'] = eod_market_cap_pivot['close index market cap'].diff().fillna(0)
# # Calculate mkt_cap_deleted_stock
# eod_market_cap_pivot['mkt_cap_deleted_stock'] = eod_market_cap_pivot.apply(
#     lambda row: row['close index market cap'] if pd.isna(row['close index market cap']) or row['close index market cap'] == 0 else 0,
#     axis=1
# )

# eod_market_cap_pivot['mkt_cap_deleted_stock'] = mkt_cap_deleted_stock.fillna(0)

# # Calculate adjusted_mkt_cap
# eod_market_cap_pivot['adjusted_mkt_cap'] = eod_market_cap_pivot['close index market cap'].shift(1) - eod_market_cap_pivot['mkt_cap_deleted_stock']
# # Calculate divisor
# eod_market_cap_pivot['divisor'] = eod_market_cap_pivot.apply(
#     lambda row: 1.00 if pd.isna(row['adjusted_mkt_cap']) or row['adjusted_mkt_cap'] == 0 else row['adjusted_mkt_cap'] / row['close index market cap'],
#     axis=1
# )
# # Calculate gross index level
# eod_market_cap_pivot['gross index level'] = eod_market_cap_pivot['close index market cap'] / eod_market_cap_pivot['divisor']

# # Calculate Index Value
# eod_market_cap_pivot['Index Value'] = eod_market_cap_pivot['gross index level'] / eod_market_cap_pivot['gross index level'].iloc[0] * 100

# # Display the DataFrame with the new columns
# eod_market_cap_pivot

